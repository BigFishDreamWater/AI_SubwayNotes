2021-1-5 从Slot Filing到RNN

假设要设计一个语音订餐系统，用户语音输入‘’今天12点吃牛肉面，不要辣椒，谢谢啦。“

我们设置几个槽位（Slot），希望算法能够将关键词'牛肉面'放入点餐食品（food）槽位，

将12点放入到达送餐时间（Time of Arrive）槽位 [, 将不要辣椒]放入要求（require）槽位。

吃放入 用户动作(user action)槽位，谢谢啦 放入其他（other）槽位。

跳过中文比较复杂的语义识别步骤，

我们假设 今天 ，12点，吃，牛肉面，不要辣椒已经分别转化成了向量。

可以使用 One-of-N Encoding 或者是 Word hashing 等编码方法，输出预测槽位的概率分布，其中预测槽位是用softmax作为多分类的概率分布。

但是这样做的话，有个问题就出现了。

如果现在用户语音输入是 “不想在今天中午吃牛肉面,中午1点来份杂酱面吧。”

这里‘牛肉面’是作为一个其他（other），所以我们应当是把杂酱面放入food槽位而不是 other槽位，

可是对于前馈网络来说，

对于同一个输入，输出的概率分布应该也是一样的，不可能出现既是food的概率最高又是other的概率最高。

所以我们就希望能够让神经网络拥有“记忆”的能力，能够根据之前的信息.

换句话说，我们希望NN能够像人一样用到句子的上下文信息来推测词语概率分布，而不是只根据这个词本身。

毕竟一词在不同语境是不同意义，而且有很多词本身就是一词多义。

------

## 2. RNN

- 基本概念

  在RNN中，隐层神经元的输出值都被保存到记忆单元中，下一次再计算输出时，隐层神经元会将记忆单元中的值认为是输入的一部分来考虑

  RNN中考虑了输入序列顺序，序列顺序的改变会影响输出的结果。

- 常见变体

  - Elman Network
    将隐层的输出（即记忆单元中的值）作为下一次的输入

  ht=σh(Whxt+Uhht−1+bh)ht=σh(Whxt+Uhht−1+bh)

  yt=σh(Wyht+by)yt=σh(Wyht+by)

  - Jordan Network
    将上一时间点的输出值作为输入

  ht=σh(Whxt+Uhyt−1+bh)ht=σh(Whxt+Uhyt−1+bh)

  yt=σh(Wyht+by)yt=σh(Wyht+by)

  - Bidirectional RNN

  - ## RNN如何学习？

    - 损失函数的定义：
      - 每一个时间点的RNN的输出和标签值的交叉熵（cross-entropy）之和
    - 训练过程：
      - 使用被称作Backpropagation through time（BPTT）的梯度下降法
      - 训练其实是比较困难的，因为Total Loss可能会出现剧烈的抖动
        ![rnn training](http://nikhilbuduma.com/img/rnn_error_surface.png)
      - 根据论文[[Razvan Pascanu,On the difficulty of training Recurrent Neural Networks,ICML'13\]](https://arxiv.org/abs/1211.5063)上对RNN的分析，损失函数
      - 的表面要么非常平坦，要么非常陡峭（The error surface is either very flat or very steep），当你的参数值在较为平坦的区域做更新时，因此该区域梯度值比较小，此时的学习率一般会变得的较大，如果突然到达了陡峭的区域，梯度值陡增，再与此时较大的学习率相乘，参数就有很大幅度更新（实线表示的轨迹），因此学习过程非常不稳定。Razvan Pascanu使用了叫做“Clipping”的训练技巧：为梯度设置阈值，超过该阈值的梯度值都会被cut，这样参数更新的幅度就不会过大（虚
      - 线表示的轨迹），因此容易收敛。
    - 为什么在RNN中会有这种问题？
      - 是因为激活函数选用了sigmoid而不是ReLU么？然而并不是。事实上，在RNN中使用ReLU反而效果会不如Sigmoid，
      - 不过也是看你的参数初始化值的选取，所以也不一定，比如后面提到的Quoc V.Le的那篇文章，使用特别初始化技巧硬训ReLU的RNN得到了可比拟LSTM的效果。因此激活函数并不是这里的关键点。
      - 那究竟是什么原因呢？我们来分析梯度更新公式中的w−η∂L∂ww−η∂L∂w来探寻一番。但是这样一个偏微分的关系我们应该如何来分析呢？这里我们用一个技巧：给w值一个微小的变化，观察对应的Loss的变化情况。假设当前模型是1000个只含有一个线性隐层的RNN级联结构。并假设我们当前的输入是100000……（只有第一个值是1，剩下全是0），
      - 因此最后的输出值是w999w999。现在假设我们ww的值是1，那么RNN在最后时间点的输出是1，给ww一个微小的变化+0.01，此时的输出变成了大约20000！这段区域呈现出一个陡峭的趋势。如果给ww一个微小的变化-0.01变为0.99，
      - 测试的输出基本变成0，哪怕是ww变到0.01时，输出依旧是0，这段区域呈现出一个平坦的趋势。因此我们可以看出由于RNN采用时间序列的结构，权重值在不同时间点被反复使用，这种累积性的变化可能对结果造成极大的影响，也可能会很长一段时间保持平稳。