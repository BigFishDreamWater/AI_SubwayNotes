### 2021-2-21 时间序列分解

关于时间序列分解，之前提到过gbdt的外推能力差，对于强趋势型序列数据建模能力不足的问题，除了差分或者增长率变换（这两个不一定有效，可能做完差分或者增长率之后还是趋势性的，即加速度都是不断增大的，并且差分和增长率在工程上的设计比较麻烦，因为你最终是要预测真实的标签，所以还需要根据某个时间点对差分进行累计计算，同时可能会出现预测出来的标签为负数的情况就很坑爹了）

------

时间序列模式一般分为：

## **趋势**

当数据长期增加或减少的情况存在的时候我们认为这个数据有趋势，趋势它不必是线性的。有时，当趋势从上升趋势变为下降趋势时，我们将其称为“变化方向”。下图是抗糖尿病药物的销售数据的趋势。



![img](https://pic4.zhimg.com/80/v2-5c578a379dc7e5cac0b454fd5121c253_720w.jpg)

## **季节性：**

时间序列受季节性因素影响，如一年的12个月或者4个季度，或者一周中的5个工作日和两个周末，**季节性总是具有固定的已知频率**。上述抗糖尿病药物的月度销售显示出季节性，这部分是由于日历年末药物成本的变化引起的。



## **周期性：**

周期性和季节性是比较容易混淆的两个概念，我们前面提到过，季节性的频率是固定的，如果数据波动的频率不是固定的，则属于周期性，通常，周期的平均长度比季节性样式的长度长，因为周期的频率不是固定的，所以周期的大小常常是变动的；



下面给出4个图：

![img](https://pic2.zhimg.com/80/v2-c098822f1647595eac398014ed7312ed_720w.jpg)

1、美国的每月的房屋销售量数据的plot，可以看到，上述数据在每年显示出明显的的季节性，**并表现出一些明显的周期性行为，周期约为6-10年**。在此期间，数据没有明显的趋势。

![img](https://pic3.zhimg.com/80/v2-1d1f6c80e545f6a99b2033eb416989d6_720w.jpg)

2、美国国库券合同数量，上图显示了1981年芝加哥市场连续100个交易日的结果，这里就没有季节性，但有明显的下降趋势。如果我们有更长的序列，我们会看到这种下降趋势实际上是一个长周期的一部分，但是当仅观察100天时，它似乎是一个趋势。

![img](https://pic2.zhimg.com/80/v2-026f58129ca2729d725dbb892d15fb29_720w.jpg)

3、澳大利亚的季度电力产量，可以看到上图显示出强劲的增长趋势，并且具有很规律的季节性，**注意这里没有任何周期性**。

![img](https://pic3.zhimg.com/80/v2-e27d080839e885e41d6e9a670e69d082_720w.jpg)

4、Google收盘价的每日变化，没有趋势，季节性或周期性行为。

------

一般来说我们是假定时间序列由 **趋势，季节性和周期性以及剩余的其它部分组成（例如重大事件等）**，只不过不同的时间序列其占比不同，比如随机波动可能完全是由残差构成的；当我们将时间序列分解为不同的components时，通常将趋势和周期组合为单个成为**趋势周期**的components（有时为简单起见也称为**趋势**）。因此，我们认为时间序列包含三个部分：趋势周期部分，季节性部分和其它部分（包含时间序列中的任何其他内容）。

一般来说，做事件序列分解有两种方式，加性和乘性：

yt=St+Tt+Rt 加性

yt=St×Tt×Rt 乘性

S代表了season 季节，t代表了trend 趋势，r代表了residual 其它难以分解无规律的部分

如果季节性波动的幅度或趋势性的强弱不随着时间的推移而发生变化则使用加法分解比较合适，如果季节性波动的幅度或者趋势性的强弱随着时间推移而发生变化（比如销量增长越来越快，销量增长曲线的斜率越来越大）则使用乘法分解比较合适，乘法分解的方式在经济学序列中很常见。

当然我们也可以先对数变换之后再分解：

yt=St×Tt×Rtis 也可以是 log(yt)=logSt+logTt+logRt.

## **这里首先介绍经典的时间序列分解方法**

也是比较常见的一种方法：

```text
import os
import numpy as np
from test_stationarity import *
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from datetime import timedelta

filename = 'api_access_fix.csv'
data = pd.read_csv(filename)
data = data.set_index('date')
data.index = pd.to_datetime(data.index)
ts = data['count']
draw_ts(ts)
```

![img](https://pic3.zhimg.com/80/v2-e8c488d37d377aae05694e524752dc02_720w.jpg)

这是原始的时间序列数据：

```text
decomposition = seasonal_decompose(data, freq=5, two_sided=False, model='Additive')
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid

from pylab import rcParams 
 
rcParams['figure.figsize'] = 11, 9 
decomposition = seasonal_decompose(data, period=5, two_sided=False, model='Additive')
fig = decomposition.plot() 
plt.show()
```

![img](https://pic1.zhimg.com/80/v2-049d8a72c2cb711092b9ddc5675c8738_720w.jpg)

进行分解之后，注意，这里是随便分解的，可以看到残差的值远大于季节性的值，显然这样的分解是不太好的，置于如何衡量分解效果的好坏后面说。

**需要注意，statsmodels的这个season_decompose不是stl分解，statsmodels的api里面包含了STL分解，但是这个season_decompose是经典分解法**；

首先是trend部分，trend部分很简单就是简单的移动平均：

![img](https://pic3.zhimg.com/80/v2-587eecd99d2e1b597e5d56578d55460a_720w.jpg)trend是调包算出来的

filt参数用于指定权重，也就是通过filt可以实现人工的加权平均，否则默认的权重是，比如period设置为5，则权重分配默认为1/5=0.2，另外就是trend是否进行插值，这个和extrapolate_trend这个参数有关，因为extrapolate_trend涉及到STL分解所以这里暂时不展开了。

然后是季节性部分，经典的时序分解方法的季节性直接是计算原始序列数据-趋势值之后的detrend值在周期内的均值.statsmodel实现的核心代码就是这一句：

```text
period_averages = seasonal_mean(detrended, period)
```

其中detrended的值为：

![img](https://pic1.zhimg.com/80/v2-51c2cced867c7b27829be0ad46f05cc0_720w.jpg)

x是原始数据，根据加性和乘性而不同。

而这个核心函数也很简单：

```text
def seasonal_mean(x, period):
    """
    Return means for each period in x. period is an int that gives the
    number of periods per cycle. E.g., 12 for monthly. NaNs are ignored
    in the mean.
    """
    return np.array([pd_nanmean(x[i::period], axis=0) for i in range(period)])
```

可以看到，计算的逻辑非常简单，举个例子，假设我们认为周期的长度为3，并且假设去除趋势之后的数据形式是：

[1,3,4,5,6,7,8]

则计算的方式就是:

(1+5+8)/3=14/3

(3+6)/2=9/2

(4+7)/2=11/2

则每个周期的值为[14/3,9/2,11/2]

然后循环即可，这样原始数据的周期seasonal值为[14/3,9/2,11/2,14/3,9/2,11/2,14/3]

就是这么简单。。。。

最后残差就是直接用原始数据减去趋势值再减去season值即可，乘性则是用除。



为啥我们需要关注时间序列的分解？首先，时间序列的分解本身就是一个很简单的单变量（自回归，这个变量是标签本身）的时间序列预测模型，season不断循环就可以得到未来的周期值了，然后trend部分不断计算移动平均就可以了，比如说我们计算出来的trend是[1,2,3]，窗口为3，那么第四个时间步的移动平均值为(1+2+3)/3=2，然后是(2+3+2(这个2是预测值))/3=7/3，然后是(3+2+7/3)/3，后面两个都是预测值。

当我们进行上述的经典方法进行时间序列分解之后，假设残差恒为0，那么我们的这个简单模型的精度就相当高了，那么原始的序列数据恰好就是由趋势+季节性构成，当然一般是不太可能，那么就出现了一个比较麻烦的实际应用的问题了，很多时候我们可以看出序列是有趋势性，有季节性的（这里的季节性不仅仅指季度，也可能是月，也可能是周的），但是实际上我们并不清楚用什么样的参数来分解比较合适，season_decompose中的参数：

![img](https://pic4.zhimg.com/80/v2-eaafcdd455ae432bb2f960f1463ba66f_720w.jpg)

model不用说了，filt用于加权，一般也不用，two_sides主要是关于怎么计算移动平均的，如果是False，则为单边计算，就比如[1,2,3,4,5]，窗口为5，则单变计算是以”5“为基准，结果为[nan,nan,nan,nan,3]，如果是双边计算，则计算是以3为基准（这称之为center 移动平均）则计算结果为[nan,nan,3,某个值，某个值],这里某个值的计算和后面的数值有关，因为这里就列了部分数值所以莫得算出来，理解意思就行了。



extrapolate_trend 和stl有关，后面讲，所以核心的参数就是period，这个period怎么确定比较合理？？

这个其实也不复杂，直观的思路就是，看最后的residual的值，这个residual肯定是越小越好了，通过residual的均值，取绝对值之后的均值，方差，画图等方式来选择一个相对最优的。

## **那么时间序列分解怎么和时间序列预测结合起来？**

这个从gbdt切入比较好理解，gbdt外推能力差对于趋势性强的时间序列数据的拟合能力比较差，通过时间序列分解之后去除了趋势性的部分，那么剩下的季节性+residual的部分，也就是简单的方法没法拟合的部分，用gbdt这样复杂的模型来拟合就比较合适了。

实际上这种做法在sktime里面实现了，不过sktime并没有进行时间序列分解，而是用多项式回归来提取趋势性数据的部分，因为对于gbdt来说，消除趋势就可以了，周期其实不需要消除，周期本身是循环的，分布是稳定的，就好像这样：

![img](https://pic3.zhimg.com/80/v2-654c182456e35aefff3959fdf5ad60ca_720w.jpg)

我们可以把上面的数据理解为去除趋势之后的剩下的season+residual的部分，那么接下来用gbdt就可以了。

关于sktime的部分：

![img](https://pic4.zhimg.com/80/v2-823a7e8e3911cd13a349c52b81bfffbf_720w.jpg)多项式回归degree=1

![img](https://pic4.zhimg.com/80/v2-673942533a8a256b7f24b2fa4cfb945f_720w.jpg)多项式回归degree=10

sktime不使用移动平均的方法来提取趋势，而是使用多项式回归，上图是多项式回归的degree为1的时候的结果，就是线性回归啦，来提取趋势，实际上我们可以自己使用sklearn的linear regression来拟合，假设序列数据有1000条，那么x我们可以设置为0~999（因为你日期没法直接拿来fit，会报错），或者把日期转化为timestamp之后进行线性回归或者lasso或者ridge回归即可。

然后预测的时候就是线性回归的趋势性部分+gbdt的预测结果。

当然，扩展开来，具有外推能力的模型，lr，nn都可以用于进行趋势的拟合，不过nn。。。应该是不行，nn直接拟合全部了。。。拟合能力太强。



针对于上面的经典的时间序列分解以及使用lr来拟合趋势，剩下的部分用复杂模型拟合的策略，谈一下缺陷部分：

1、经典的时间序列分解方法的精度太差了，很多场景不适用，比如电商销量，显然有很明显的节假日效应，这个压根分解不出来，像双十一，618日期前后部分的residual会特别大；

2、经典的时间序列分解方法的季节性分解做的比较简单，每一个周期内的值都是不变的，比如上面的例子，周期永远是：[14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2][14/3,9/2,11/2]，，，，，，一些场景下效果会比较渣，例如，随着空调的普及，电力需求模式已经随着时间而改变。具体而言，在许多地区，几十年前的季节性使用模式在冬季（由于采暖）达到了最大需求，而当前的季节性模式在夏季（由于空调）而达到了最大需求。**经典的分解方法无法捕获随时间变化的这些季节性变化**。

3、多个季节性嵌套的时间序列数据的问题，上述经典的时间序列分解方法，通过period的参数仅仅是指定了一个季节性的周期，如果时间序列本身是多个season的叠加，则上述方法难以适用；

4、lr+gbdt的策略，避开了季节性周期的计算，单独对趋势进行拟合，最大的问题就是异常样本很难处理，并且实际上异常样本非常容易影响lr的训练结果，而这一点对于电商销量来说是很致命的，因为双十一的销量就是一个异常点，根据mse的损失函数，异常样本会不断的被模型训练学习，难以收敛，同时导致了整个拟合平面的偏移，泛化性能会变得比较渣，

![img](https://pic1.zhimg.com/80/v2-0160269809a942ac58df558fcd99cc34_720w.jpg)

比如说上图，去掉异常样本之后，lr能够较好的拟合，如果加入了异常样本，则lr最终拟合出来的平面就是绿线这个鬼样子压根没法用，泛化性能很差，误差很大；

5、电商序列预测的一大难点就在于不同城市，甚至是不同商店，不同sku的序列数据，其趋势性和周期性都可能是不同的，这就意味着我们可能需要为每一个”单位“单独去拟合一个lr模型，显然，不太可能，维护成百上前个模型是自找麻烦；

------

休息。。。。

