谈谈决策树：

决策树是一个判别式模型，基于树结构进行决策，其中root结点是全体样本集，中间结点是遍历样本不同特征测试出来的属性最优的子样本集，叶子结点是最终的判定结果。

核心在于 如何选择最优的划分属性。

通常来说，有根据信息增益，信息增益比，和Gini信息。

信息增益就是划分前后信息熵之差，信息熵的公式 -p*log p

ID3 决策树的生成过程，要求特征必须离散，然后遍历每一个特征，计算每一个离散特征不同取值划分后的信息增益，贪心的选择最大的那个特征，不考虑全局最优。将树划分为对应不同子树，然后递归进行。

缺点1.如果一个离散特征有多个取值，考虑极端情况下是ID类这种特征， 决策树会更倾向于选择这种特征，极容易发生过拟合

缺点2:无法处理连续特征。

缺点3.没有处理缺失值。

缺点4：没有剪枝策略，容易过拟合，泛化性能差。

缺点5：特征无法复用，上一次使用的特征不能再用。

缺点6.log计算量偏大

缺点7：无法进行回归类型预测。





C4.5：主要改进就是将属性划分改为信息增益比，就是gain/IV(A)，特征取值越大，则IV越大。相当于信息增益这个损失函数加了一个结点数的正则，这样就能一定程度抑制离散特征取值较多导致划分结点过多的情况。

当然可以看到信息增益比更偏向于属性值较小的，所以为了平衡，遍历的时候C4.5会先选取信息增益高于平均的，然后从中再选择信息增益比最高的。

第二个增加对连续特征处理的算法，就是将连续特征取值排序，取相邻两个样本的均值作为划分点，然后依次遍历所有划分点，将大于划分点和小于划分点的样本区间进行信息增益比计算，最终选择属性收益最大的那个划分点作为划分点。

第三个增加对缺失值的处理：

第四个使用 悲观后剪枝策略。

消除缺点1，2，3，4 并且连续特征的是可以二次复用的。



Card树：

Gini系数：p*(1-p) 越小越好。 是熵模型的泰勒一阶展开。



决策树的剪枝：无论以上怎么调整，都很难避免决策树是容易过拟合的模型。

包含预剪枝和后剪枝。

预剪枝就是生成树的根据一些规则停止生长，比如数据样本低于一定阈值，划分后指标增益低于某个阈值就不用划分等等。

而后剪枝则是建树完成后，对树