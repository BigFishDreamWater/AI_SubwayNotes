### 2021-1-25 seq2seq到transformer

一句话总结：

初期的DNN很难解决slot_filing这种需要上下文信息的NLP和涉及到时间序列有先后隐状态的时序问题，

所以出现了RNN。

而简单RNN有三大问题：

1. 需要输入和输入同长度，也就是N2N，但很多情况输入和输出不一定一样，比如中文翻译到英语这种场景

2. 第二个就是RNN的训练只能穿行，不能并行，无法利用分布式计算资源来加速计算。像是google这种分布式计算资源丰富的就很郁闷。

3. 第三个利用的上下文信息比较短，即便后续有LSTM这样的改进，但依然还是不太够，稍微长一点的距离信息就很难发掘出来。而且一个序列中不同token的带来信息的权重应该是不一样的。而RNN是无法提取出这种权重信息的。

N2N的解决则是从N21到12N再到N2M。

其中N2M就是所谓的seq2seq，也就是encoder to decoder。

做DNN的这些学者就喜欢取听起来高大上，非常唬人的名词。

思想非常简单，既然一个RNN只能N2N，或者N21,或者12N

那么就设计两个2个RNN，第一个RNN将N个长度序列输出为一个S长度的向量，也就是N21

第二个RNN再将S长度的向量转化为M个长度的序列。也就是12M。

至于并行训练，在attention没有出来之前，其实可以通过CNN来解决，但是CNN的参数过多，且实际效果并不是特别的理想。

终于在2017年，上下文信息的挖掘和并行话计算就是self-attention机制，产出于google的一篇论文，attention is all you need。

这个通过QKV的向量匹配，同时解决了并行化和全局信息提取的问题。

transformer就是一个加了self-attention机制的seq2seqmodel。

而这三个问题的解决，就好像从原始人终于学会了用锤子和认字，而大家对这个原始人的期待并不是用一大堆锤子来写字，

而是表演超级智能这样的绝世武功。

并行化和全局信息的提取基本让NLP的道路走的差不多了，剩下的无非就是更多的计算资源和更多的标记数据



