### 2021-2_5_google GFS   map_reduce  big_table 细读论文之GFS

GFS

google面临的问题在数据量巨大时候的形成 数据存储，数据计算，以及数据调用的三个大难题。

GFS以分布式文件系统解决了数据存储的问题，能将海量数据存储在数以百万的廉价磁盘上。

以此产生的开源分布式文件系统HDFS几乎是现在大数据存储系统的唯一有效解决方案。

而map_reduce变成模型则解决了集群对海量数据的并行计算问题，其中的map,shuffle,reduce计算思想是所有分布式计算框架的核心思想，无论是spark还是hadoop。

big_table 则是对分布式数据库的一次探索落地验证，不算是特别完美，google自己也在后续重新实现另外一种分布式数据库 spanner.

但是它的思想内容产生hbase这个在早些年广泛运用的大型数据仓库。

精读google三架马车，是大数据从业者的必备功课。

先说GFS

设想一个问题。存在一个数据高达100T，单机几乎没办法找到一个硬盘存入如此大的文件，即便有，价格也是非常高。

当然，1T的硬盘就比较常见了。

那么如何将100T数据存入到100个 各自带有1T硬盘的集群当中。

显而易见且唯一的办法 就是将数据分为100份，每份1T，然后分别存入集群的每个结点磁盘中。

当然也不一定分成100份，你可以选择分101份，200份等等，不管怎么分，总之就是大数化小，分而存之。

存好了，又面临一个问题，就是有一台机器的硬盘突然出现了意外，上面的数据都没了，也没办法恢复了。

再严重点，如果100T是内容具有连续性质的文件，比如说是100T的VR电影或者游戏或者代码之类的，少了1T完全没办法运行，会导致其他结点数据的失效。

那怎么办？

 这是集群中非常常见的问题，

常见到GFS论文已经将机器故障设定为必然发生的事件，而不是偶发事件。

你可以假定集群中每天必然有一台机器要莫名其妙的挂掉。

所以必须有一种方案来解决掉 各种数据意外丢失，或者读不了的情况。

能够解决这样的办法，对于原始数据来说。

其实有且只有一个，那就是冗余数据。

也就是数据复制多份，放在不同的地方，如果有一个地方出了问题，就用另外一个地方的备份数据来顶上。

就好像我们经常做的操作，好的视频不仅本地电脑留一份，百度网盘也要留一份，自己的外置硬盘也要留一份。

GFS的内容就是 如何又划算又安全又高效的将大文件存储在集群上的不同机器上。

下面开始细扣GFS论文的细节：

摘要：

惯例‘吹嘘’ 

我们设计了一个和传统分布式文件系统不同的弹性分布式文件系统，

高容错，高性能，关键还很便宜。

而且我们不是谈理论，我们是已经开发出来并且已经大规模使用在我们的系统了，效果nice。

简介：也就是 被认为是需求开发文档，在设计GFSgoogle重点考虑的问题。

1. 组件失效被认为是常态事件，而不是意外事件。 所以监控系统，错误侦测，灾难荣誉和自动恢复是必备系统。

2. 我们存储的文件集合很大，GB文件是常态，甚至还有TB级别的。但是把，这些文件集合通常包含了都是几KB或者十几KB的小文件，比如网页文档缩略。如果说文件系统覆盖管理到KB级别的文件的话，数量过于庞大，管不过来了。太小，或者太大都行，所以我们应该合理的设定文件的最小存储级别。

3. 绝大部分文件修改都在文件尾部追加，而不是采取覆盖方式或者随机写入。

   追加数据的操作就保证了数据操作的原子性，要么追加成功，要么追加失败。

    且性能也非常的棒。

4. 这个原因是google业务场景所致，因为他们的搜索引擎爬虫爬取网页进行索引正排

   和索引倒排的存储都是一次存储，后续是不会修改的。

   存储完之后，文件只有读，而且大多数业务是顺序读，不会随机读。

   所以重点优化实现顺序读的性能，可以牺牲随机读的性能。

4. 基于只专注文件尾部追加操作，我们就可以在多个客户端结点进行并行的写入数据

   通过这种折中方式保证了数据的一致性。 

   这种一致性要求不是很高，但是大大减少了整个系统的式设计难度。

   最直观的设计就是文件系统的API能够直接同意作用在每一个应用程序上。

   每个应用程序都调用相同的文件接口进行文件处理，而不用对每一个程序单独定制它的文件处理接口来保证一致性。

   由此可以学习到，解决一个需求问题的一个思路就是将问题简化，只解决必须要解决的问题，剩下的问题都可以退让甚至直接牺牲掉。

   

2.1 设计概述：

1. 总结需求设计， 

   1. 容灾是必须的。包含系统监控，灾难识别，数据备份，数据恢复 四个方面。

   2. 文件管理，主要针对大文件管理优化，小文件也有，但是不需要专门的优化。

   3. 文件读操作。根据经验，分布式文件系统的读操作主要是两种：大规模的流式读取和小规模的随机读取。流式读取通常都是连续区域读取，

      随机读取的话 不保证性能，如果程序非要求随机读取性能，那么就随机读取操作进行合并排序，然后再按照顺序读取。

   4. 文件写操作，重点优化支持追加写入，小规模随机写入也可以，但是不保证性能。至于修改的话，需求不大，凑合做一下就行。

   5. 文件的追加写入操作优先级最高，尤其在多个客户端对同一个文件又是读又是写的时候，那么

      一定是写入后再读取。你可以写完后再读取，也可以一边写入一遍读，总之保证多客户端同步追加数据的明确性。

   6. 总的来说，我们要求就是高速，大批量的处理数据，对于单一的读写操作就顺其自然了。

      

   2.接口设计

   1.文件以分层目录树的形式组织，路径名标识，支持新建，删除打开，关闭，读写文件

   2.提供快照和记录追加操作。快照是很低的成本创建文件和目录树的拷贝。记录追加

   则是允许多个客户端对同一个文件进行数据追加，在不需要额外同步锁定的情况下ia，保证追加操作是原子性的。

   

   3.架构设计

   